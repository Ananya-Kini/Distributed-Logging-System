## Distributed Logging System

## Project Overview
In a microservices architecture, effective log management is crucial for operational excellence. This project aims to streamline the collection, storage, and analysis of logs generated by various services, enhancing the ability to track application behavior and identify errors quickly. By capturing relevant metadata alongside each log entry and enabling real-time ingestion and querying, the system improves operational visibility and facilitates proactive responses to potential issues. Ultimately, this distributed logging framework enhances resilience and maintainability in a dynamic application landscape.

## System Architecture and Flow
![[bdarch.drawio.png]]
- **Microservices (Processes)**: Represent distributed nodes that independently generate logs and send heartbeat signals to monitor their status.
- **Log Accumulator**: Collects log data from each node, structures it, and forwards it to the Pub-Sub model for centralized log management.
- **Pub-Sub Model**: Acts as a communication layer, facilitating reliable, asynchronous distribution of logs.
- **Log Storage**: A system for indexing and storing logs in a searchable format for easy access and monitoring.
- **Alerting System**: Listens for specific log levels (e.g., ERROR, FATAL, etc.) in real-time, generating alerts to ensure prompt responses to critical events.
- **Heartbeat Mechanism**: Provides failure detection by alerting when a node stops sending heartbeats, signaling that the node may have failed.

## Guide to run the Distributed Logging System
Pre-requisites : Sqlite, Fluentd, Kafka, Elasticsearch, Kibana and a few python modules. 
Steps to run the distributed logging system: 
On consumer side (with pubsub model):

1. Run ```ifconfig```and obtain the its IP address.

2. Run ```sudo python3 spGen.py "IP_ADDR" "0/1"``` on it and put the the IP address from the previous step. Use 0 if your server.properties is in ```/usr/local/kafka/config``` and 1 if server.properties is in ```/opt/kafka/config``` 

3. Restart Kafka using ```sudo systemctl restart kafka```

On the application side : 

1. Run ```sudo python3 fcGen.py "IP_ADDR" ``` on it and put the the IP address of the consumer. 

2. Restart Fluentd using sudo systemctl restart fluentd

After these initial IP configuration steps, we proceed to running the microservice architecture and logging. 

1. Run ```python3 pubsub.py``` , ```python3 filterer.py``` ```python3 alertingsystem.py``` on three separate terminals in the consumer side. 

2. Enter into the microservice(s) that you wish to use and run ```uvicorn main:app --reload``` in order to start the application. Go to ```127.0.0.1/docs``` for an easier interface to make requests. 

3. All the three Kafka consumers will display the logs on the terminals. PubSub model sends the logs to the particular topics : all_logs, critical_logs and node_failure.  Filterer displays all the logs except the heartbeat, and these logs are stored in Elasticsearch, and can be viewed in Kibana. Alerting System only displays the critical_logs and node_failure. 
